{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curso de Optimización (DEMAT)\n",
    "## Tarea 5\n",
    "\n",
    "| Descripción:                         | Fechas               |\n",
    "|--------------------------------------|----------------------|\n",
    "| Fecha de publicación del documento:  | **Marzo  6, 2022**   |\n",
    "| Fecha límite de entrega de la tarea: | **Marzo 13, 2022**   |\n",
    "\n",
    "\n",
    "### Indicaciones\n",
    "\n",
    "Puede escribir el código de los algoritmos que se piden en una\n",
    "celda de este notebook o si lo prefiere, escribir las funciones\n",
    "en un archivo `.py` independiente e importar la funciones para\n",
    "usarlas en este notebook. Lo importante es que en el notebook\n",
    "aparezcan los resultados de la pruebas realizadas y que:\n",
    "\n",
    "- Si se requieren otros archivos para poder reproducir los resultados,\n",
    "  para mandar la tarea cree un archivo ZIP en el que incluya\n",
    "  el notebook y los archivos adicionales. \n",
    "- Si todos los códigos para que se requieren para reproducir los\n",
    "  resultados están en el notebook, no hace falta comprimirlo \n",
    "  y puede anexar sólo el notebook en la tarea del Classroom.\n",
    "- Exportar el notebook a un archivo PDF y anexarlo en la tarea del\n",
    "  Classroom como un archivo independiente.\n",
    "  **No lo incluya dentro del ZIP**, porque la idea que lo pueda accesar \n",
    "  directamente para poner anotaciones y la calificación de cada ejercicio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ejercicio 1 (6 puntos)\n",
    "\n",
    "Programar el método de descenso máximo con tamaño de paso fijo y probarlo.\n",
    " \n",
    "El algoritmo recibe como parámetros la función gradiente $g(x)$ de la función objetivo,\n",
    "un punto inicial $x_0$, el valor del tamaño de paso $\\alpha$, un número  máximo de iteraciones $N$, \n",
    "la tolerancia $\\tau>0$. Fijar $k=0$ y repetir los siguientes pasos:\n",
    "\n",
    "1. Calcular el gradiente $g_k$ en el punto $x_k$, $g_k = g(x_k)$.\n",
    "2. Si $\\|g_k\\| < \\tau$,  hacer  $res=1$ y terminar. \n",
    "3. Elegir la dirección de descenso como $p_k = - g_k$.\n",
    "4. Calcular el siguiente punto de la secuencia como\n",
    "   $$x_{k+1} = x_k + \\alpha p_k $$\n",
    "5. Si $k+1\\geq N$, hacer $res=0$ y terminar.\n",
    "6. Si no, hacer $k = k+1$ y volver el paso 1.\n",
    "7. Devolver el punto $x_k$,  $g_k$, $k$ y $res$.\n",
    "\n",
    "---\n",
    "\n",
    "De acuerdo con la proposición vista en la clase 12, para que el método de\n",
    "máximo descenso con paso fijo para funciones cuadráticas converja \n",
    "se requiere que el tamaño de paso $\\alpha$ cumpla con\n",
    "\n",
    "$$ \n",
    "0 < \\alpha < \\frac{2}{\\lambda_{\\max}(A)} = \\alpha_{\\max}, \n",
    "$$\n",
    "\n",
    "donde $\\lambda_{\\max}(A)$ es el eigenvalor más grande de $A$.\n",
    "\n",
    "1. Escriba una función que implementa el algoritmo de descenso máximo\n",
    "   con paso fijo.\n",
    "   \n",
    "2. Programe  las funciones cuadráticas y sus gradientes \n",
    "   \n",
    "   $$ \n",
    "   f_i(x) = \\frac{1}{2} x^\\top \\mathbf{A}_i x - \\mathbf{b}^\\top_i x, \\quad i=1,2 \n",
    "   $$\n",
    "   \n",
    "   donde \n",
    "\n",
    "   $$ \n",
    "   \\mathbf{A}_1 = \\left[ \\begin{array}{cc}\n",
    "   1.18 & 0.69 \\\\\n",
    "   0.69 & 3.01 \n",
    "   \\end{array} \\right],\\qquad \n",
    "   \\mathbf{b}_1 = \\left( \\begin{array}{r} -0.24 \\\\ 0.99 \\end{array} \\right).\n",
    "   $$\n",
    "   \n",
    "$$ \n",
    "\\mathbf{A}_2 = \\left[ \\begin{array}{ccccc}\n",
    "6.36 & -3.07 & -2.8  & -3.42 & -0.68 \\\\\n",
    "-3.07 & 10.19 &  0.74 &  0.5  &  0.72 \\\\\n",
    "-2.8  & 0.74  &  4.97 & -1.48 &  1.93 \\\\\n",
    "-3.42 & 0.5   & -1.48 &  4.9  & -0.97 \\\\\n",
    "-0.68 & 0.72  &  1.93 & -0.97 &  3.21 \n",
    "\\end{array} \\right],\\qquad \n",
    "\\mathbf{b}_2 \n",
    "= \n",
    "\\left( \\begin{array}{r} 0.66 \\\\ 0.37  \\\\ -2.06  \\\\ 0.14 \\\\ 1.36 \\end{array} \\right).\n",
    "$$\n",
    "\n",
    "3. Fije el número máximo de iteraciones $N=2000$ y la tolerancia $\\tau =\\sqrt{\\epsilon_m}$,\n",
    "   donde $\\epsilon_m$ es el épsilon de la máquina.\n",
    "   Para cada función cuadrática, calcule $\\alpha_{\\max}$ de la matriz $\\mathbf{A}_i$. \n",
    "   Pruebe con los tamaños de paso $\\alpha$ igual a $1.1\\alpha_{\\max}$ y $0.9\\alpha_{\\max}$.\n",
    "   Use el punto inicial \n",
    "   \n",
    "$$\n",
    "\\mathbf{x}_0 = \n",
    "\\left( \\begin{array}{r} -38.12 \\\\ -55.87  \\end{array} \\right) \\quad \\text{para} \\quad f_1\n",
    "$$\n",
    "   \n",
    "$$\n",
    "\\mathbf{x}_0 = \n",
    "\\left( \\begin{array}{r} 4.60 \\\\  6.85 \\\\  4.31 \\\\  4.79 \\\\  8.38  \n",
    "\\end{array} \\right) \\quad \\text{para} \\quad f_2\n",
    "$$\n",
    "   \n",
    "4. En cada caso imprima $x_k$, $\\|g_k\\|$, el número de iteraciones $k$ y \n",
    "   el valor de $res$.\n",
    "\n",
    "\n",
    "### Solución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent_fixed_step(gf, x_0, alpha, N, T):\n",
    "\n",
    "    x_k = x_0\n",
    "    k = 0\n",
    "    res = 0\n",
    "    while (k < N):\n",
    "        gf_k = gf(x_k)\n",
    "        if np.linalg.norm(gf_k) < T:\n",
    "            res = 1\n",
    "            break\n",
    "        p_k = -gf_k\n",
    "        a_k = alpha\n",
    "        x_k = x_k + a_k*p_k\n",
    "        k = k + 1\n",
    "\n",
    "    return x_k, gf(x_k), k, res\n",
    "\n",
    "def test(g, x_0, alpha, N, T):\n",
    "    x_k, g_k, k, res = gradient_descent_fixed_step(g, x_0, alpha, N, T)\n",
    "    print(f\"El algoritmo \" + (\"no \" if res == 0 else \"\") + \"convergió en:\")\n",
    "    x_output = np.concatenate((x_k[0,:3], x_k[0,-min(3, len(x_k)-3):]if len(x_k) > 3 else []))\n",
    "    print(f\"k = {k}, ||g_k|| = {np.linalg.norm(g_k)}, x_k = {x_output}\")\n",
    "\n",
    "def square_function_test(A, b, x_0, alpha, N, T):\n",
    "    test((lambda x : x@A.T - b), x_0, alpha, N, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El algoritmo no convergió en:\n",
      "k = 2000, ||g_k|| = inf, x_k = [-4.77996787e+159 -1.42775834e+160]\n",
      "El algoritmo convergió en:\n",
      "k = 105, ||g_k|| = 1.413707145775169e-08, x_k = [-0.45696914  0.43365738]\n",
      "El algoritmo no convergió en:\n",
      "k = 2000, ||g_k|| = inf, x_k = [-7.41437598e+158  9.63088421e+158  3.28353818e+158]\n",
      "El algoritmo convergió en:\n",
      "k = 1064, ||g_k|| = 1.471192113738863e-08, x_k = [-2.77194407 -0.52190805 -3.05959477]\n"
     ]
    }
   ],
   "source": [
    "# Pruebas del algoritmo\n",
    "import itertools\n",
    "\n",
    "A = [\n",
    "    np.array(\n",
    "        [\n",
    "            [1.18, 0.69],\n",
    "            [0.69 , 3.01]\n",
    "        ]\n",
    "    ),\n",
    "    np.array(\n",
    "        [\n",
    "            [6.36  , -3.07 , -2.8  , -3.42 , -0.68 ],\n",
    "            [-3.07 , 10.19 ,  0.74 ,  0.5  ,  0.72],\n",
    "            [-2.8  , 0.74  ,  4.97 , -1.48 ,  1.93],\n",
    "            [-3.42 , 0.5   , -1.48 ,  4.9  , -0.97],\n",
    "            [-0.68 , 0.72  ,  1.93 , -0.97 ,  3.21 ]\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "b = [\n",
    "        np.array(\n",
    "            [[-0.24, 0.99]]\n",
    "        ),\n",
    "        np.array(\n",
    "            [[0.66, 0.37, -2.06, 0.14, 1.36]]\n",
    "        ),\n",
    "]\n",
    "\n",
    "x = [\n",
    "        np.array(\n",
    "            [[-38.12, -55.87]]\n",
    "        ),\n",
    "        np.array(\n",
    "            [[4.60,  6.85,  4.31,  4.79,  8.38]]\n",
    "        )\n",
    "]\n",
    "\n",
    "for (A_i, b_i, x_i), fact in itertools.product(zip(A,b,x),[1.1,.9]):\n",
    "    eig_vals = np.linalg.eigvals(A_i)\n",
    "    eig_vals = eig_vals[np.isreal(eig_vals)].real\n",
    "    square_function_test(A_i, b_i, x_i, fact*(2/max(eig_vals)), 2000, np.finfo(float).eps**(1/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comentarios sobre las trayectorias:\n",
    "\n",
    "No convergio el algoritmo, cuando tomamos un paso fijo mayor a $\\frac{2}{\\lambda_{\\max}(A)}$ como habiamos demostrado, pero si converge cuando es menor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2 (4 puntos)\n",
    "\n",
    "Pruebe el método de descenso máximo  con paso fijo aplicado a la \n",
    "función de Rosenbrock.\n",
    "\n",
    "Encuentre un valor adecuado para $\\alpha$ para que el algoritmo \n",
    "converja. Use como punto inicial el punto $(-12, 10)$.\n",
    "\n",
    "Imprima $x_k$, $\\|g_k\\|$, el número de iteraciones $k$ y el valor de $res$.\n",
    "\n",
    "\n",
    "### Solución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En esta celda puede poner el código de las funciones\n",
    "# o poner la instrucción para importarlas de un archivo .py\n",
    "def f_rosenbrock(x):\n",
    "    return 100*(x[0,1] - x[0,0]**2)**2 + (1 - x[0,0])**2\n",
    "\n",
    "def gf_rosenbrock(x):\n",
    "    return np.array( [[400*(x[0,0]**3-x[0,0]*x[0,1])+2*(x[0,0]-1), 200*(x[0,1]-x[0,0]**2)]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\batma\\AppData\\Local\\Temp\\ipykernel_10164\\2467917378.py:7: RuntimeWarning: overflow encountered in double_scalars\n",
      "  return np.array( [[400*(x[0,0]**3-x[0,0]*x[0,1])+2*(x[0,0]-1), 200*(x[0,1]-x[0,0]**2)]] )\n",
      "C:\\Users\\batma\\AppData\\Local\\Temp\\ipykernel_10164\\2467917378.py:7: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return np.array( [[400*(x[0,0]**3-x[0,0]*x[0,1])+2*(x[0,0]-1), 200*(x[0,1]-x[0,0]**2)]] )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El algoritmo convergió en:\n",
      "k = 1904, ||g_k|| = 1.4808304112176072e-08, x_k = [0.99999999 0.99999998]\n"
     ]
    }
   ],
   "source": [
    "# Pruebas realizadas a la función de Rosenbrock\n",
    "tl=0\n",
    "tr=1\n",
    "convergence=True\n",
    "ans=0\n",
    "x_0 = np.array([[-12,10]])\n",
    "while(convergence):\n",
    "  trd1 = tl+(tr-tl)/3\n",
    "  trd2 = tl+2*((tr-tl)/3)\n",
    "  ans1 = gradient_descent_fixed_step(gf_rosenbrock, x_0, trd1, 20000, np.finfo(float).eps**(1/2))\n",
    "  ans2 = gradient_descent_fixed_step(gf_rosenbrock, x_0, trd2, 20000, np.finfo(float).eps**(1/2))\n",
    "  if(ans1[3]==1):\n",
    "    convergence = False\n",
    "    ans = trd1\n",
    "  if(ans2[3]==1):\n",
    "    convergence = False\n",
    "    ans = trd2\n",
    "  if(np.linalg.norm(ans1[1]) > np.linalg.norm(ans2[1])):\n",
    "    tl = trd1\n",
    "  else:\n",
    "    tr = trd2\n",
    "test(gf_rosenbrock, x_0, trd1, 20000, np.finfo(float).eps**(1/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se uso una busqueda ternaria para encontrar el paso adecuado para que el descenso por gradiente con paso fijo converja con la función Rosenbrock."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
