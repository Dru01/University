{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curso de Optimización (DEMAT)\n",
    "## Tarea 8\n",
    "\n",
    "| Descripción:                         | Fechas               |\n",
    "|--------------------------------------|----------------------|\n",
    "| Fecha de publicación del documento:  | **Abril 11, 2022**   |\n",
    "| Fecha límite de entrega de la tarea: | **Mayo   1, 2022**   |\n",
    "\n",
    "\n",
    "### Indicaciones\n",
    "\n",
    "- Envie el notebook que contenga los códigos y las pruebas realizadas de cada ejercicio.\n",
    "- Si se requiren algunos scripts adicionales para poder reproducir las pruebas,\n",
    "  agreguelos en un ZIP junto con el notebook.\n",
    "- Genere un PDF del notebook y envielo por separado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ejercicio 1 (3 puntos)\n",
    "\n",
    "Sea $x=(x_1, x_2, ..., x_n)$ la variable independiente.\n",
    "\n",
    "Programar las siguientes funciones y sus gradientes:\n",
    "\n",
    "- Función cuadrática \n",
    "\n",
    "$$ f(\\mathbf{x}) = 0.5\\mathbf{x}^\\top \\mathbf{A}\\mathbf{x} - \\mathbf{b}^\\top\\mathbf{x}. $$\n",
    "\n",
    "Si $\\mathbf{I}$ es la matriz identidad y $\\mathbf{1}$ es la matriz llena de 1's,\n",
    "ambas de tamaño $n$, entonces\n",
    "\n",
    "$$ \\mathbf{A} = n\\mathbf{I} + \\mathbf{1} = \n",
    "\\left[\\begin{array}{llll} n      & 0      & \\cdots & 0 \\\\\n",
    "                       0      & n      & \\cdots & 0 \\\\ \n",
    "                       \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                       0      & 0      & \\cdots & n \\end{array}\\right]\n",
    "+ \\left[\\begin{array}{llll} 1    & 1      & \\cdots & 1 \\\\\n",
    "                       1      & 1      & \\cdots & 1 \\\\ \n",
    "                       \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                       1      & 1      & \\cdots & 1 \\end{array}\\right],  \\qquad\n",
    "\\mathbf{b} = \\left[\\begin{array}{l} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right] $$\n",
    "\n",
    "\n",
    "- Función generalizada de Rosenbrock\n",
    "\n",
    "$$  f(x) = \\sum_{i=1}^{n-1} 100(x_{i+1} - x_i^2)^2 + (1 - x_{i} )^2  $$\n",
    "\n",
    "$$ x_0 = (-1.2, 1, -1.2, 1, ..., -1.2, 1) $$\n",
    "\n",
    "\n",
    "En la implementación de cada función y de su gradiente, se recibe como argumento la variable $x$\n",
    "y definimos $n$ como la longitud del arreglo $x$, y con esos datos aplicamos la \n",
    "definición correspondiente.\n",
    "\n",
    "Estas funciones van a ser usadas para probar los algoritmos de optimización.\n",
    "El  punto $x_0$ que aparece en la definición de cada función es el punto inicial\n",
    "que se sugiere para el algoritmo de optimización.\n",
    "\n",
    "\n",
    "### Solución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Implementación de la función cuadrática y su gradiente\n",
    "\n",
    "def cuad(x):\n",
    "    N = x.shape[1]\n",
    "    \n",
    "    b = np.ones((1,N))\n",
    "    A = np.identity(N)*N + np.ones((N,N))\n",
    "    return 0.5 * x @ A @ x.T - b @ x.T\n",
    "\n",
    "def g_cuad(x):\n",
    "    N = x.shape[1]\n",
    "    \n",
    "    b = np.ones((N,))\n",
    "    A = np.identity(N)*N + np.ones((N,N))\n",
    "    \n",
    "    return x @ A - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 4 5]]\n",
      "[[2 3 4 5]] [[1 2 3 4]]\n"
     ]
    }
   ],
   "source": [
    "# Implementación de la función tridiagonal generalizada y su gradiente\n",
    "import numpy as np\n",
    "\n",
    "N = 5\n",
    "\n",
    "b = np.ones((1,N))\n",
    "x = np.array([[1,2,3,4,5]])\n",
    "print(x)\n",
    "\n",
    "print(x[:,1:], x[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Implementación de la función generalizada de Rosenbrock y su gradiente\n",
    "\n",
    "def rosenbrock(x):\n",
    "    x1 = x[:,:-1]\n",
    "    x2 = x[:,1:]\n",
    "    return np.sum(100 * (x2 - x1**2)**2 + (1-x1)**2)\n",
    "    \n",
    "def g_rosenbrock(x):\n",
    "    N = x.shape[1]\n",
    "    \n",
    "    x1 = x[:,:-1]\n",
    "    x2 = x[:,1:]\n",
    "    \n",
    "    d1 = -400 * (x2 - x1**2) * x1 - 2 * (1 - x1)\n",
    "    d2 = 200 * (x2 - x1**2)\n",
    "    \n",
    "    d = np.zeros((1,N))\n",
    "    d[:,0] = d1[:,0]\n",
    "    d[:,1:-1] = d1[:,1:] + d2[:,:-1]\n",
    "    d[:,-1] = d2[:,-1]\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2 (3.5 puntos)\n",
    "\n",
    "Programar el método de gradiente conjugado no lineal de Fletcher-Reeves:\n",
    "\n",
    "---\n",
    "\n",
    "La implementación recibe como argumentos a la función objetivo $f$, su gradiente $\\nabla f$,\n",
    "un punto inicial $x_0$, el máximo número de iteraciones $N$ y una tolerancia $\\tau>0$.\n",
    "\n",
    "1. Calcular  $\\nabla f_0 = \\nabla f(x_0)$, $p_0 = -\\nabla f_0$ y hacer $res=0$.\n",
    "2. Para $k=0,1,..., N$:\n",
    "\n",
    "- Si $\\|\\nabla f_k\\|< \\tau$, hacer $res=1$ y terminar el ciclo\n",
    "- Usando backtracking calcular el tamaño de paso  $\\alpha_k$\n",
    "- Calcular $x_{k+1} = x_k + \\alpha_k p_k$\n",
    "- Calcular $\\nabla f_{k+1} = \\nabla f(x_{k+1})$\n",
    "- Calcular \n",
    "\n",
    "$$ \\beta_{k+1} = \\frac{\\nabla f_{k+1}^\\top \\nabla f_{k+1}}{\\nabla f_{k}^\\top\\nabla f_{k}}  $$ \n",
    "\n",
    "- Calcular \n",
    "\n",
    "$$ p_{k+1} = -\\nabla f_{k+1} + \\beta_{k+1} p_k $$\n",
    "\n",
    "3. Devolver $x_k, \\nabla f_k, k, res$\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "---\n",
    "\n",
    "1. Escriba la función que implemente el algoritmo anterior.\n",
    "2. Pruebe el algoritmo usando para cada una de las funciones del \n",
    "   Ejercicio 1, tomando el punto $x_0$ que se indica.\n",
    "3. Fije $N=50000$, $\\tau = \\epsilon_m^{1/3}$.\n",
    "4. Para cada función del Ejercicio 1 cree el punto $x_0$ correspondiente\n",
    "   usado $n=2, 10, 20$ y ejecute el algoritmo.\n",
    "   Imprima\n",
    "   \n",
    "- n,\n",
    "- f(x0),\n",
    "- las primeras y últimas 4 entradas del punto $x_k$ que devuelve el algoritmo,\n",
    "- f(xk),\n",
    "- la norma del vector $\\nabla f_k$, \n",
    "- el  número $k$ de iteraciones realizadas,\n",
    "- la variable $res$ para saber si el algoritmo puedo converger.\n",
    "  \n",
    "\n",
    "### Solución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En esta celda puede poner el código de las funciones\n",
    "# o poner la instrucción para importarlas de un archivo .py\n",
    "\n",
    "def backtracking(f, xk, pk, p, beta):\n",
    "    alpha = 1\n",
    "    fk = f(xk)\n",
    "    while True:\n",
    "        if f(xk + alpha * pk) <= fk - beta*alpha*(pk @ pk.T):\n",
    "            return alpha\n",
    "        alpha = p * alpha\n",
    "\n",
    "def Fletcher_Reeves(f, g, x0, maxN, tol):\n",
    "    xk = x0\n",
    "    res = 0\n",
    "    \n",
    "    gnext = g(xk)\n",
    "    pk = -gnext\n",
    "    \n",
    "    for k in range(maxN):\n",
    "        gk = gnext\n",
    "        if np.linalg.norm(gk) < tol:\n",
    "            res = 1\n",
    "            break\n",
    "\n",
    "        alpha = backtracking(f, xk, pk, 0.8, 0.0001)\n",
    "        xk = xk + alpha * pk\n",
    "        \n",
    "        gnext = g(xk)\n",
    "        betak = (gnext @ gnext.T) / (gk @ gk.T)\n",
    "        pk = -gnext + betak * pk\n",
    "        \n",
    "        if np.abs(gnext @ gk.T) > 0.2 * np.linalg.norm(gnext)**2:\n",
    "            # Reinicio\n",
    "            pk = - gnext\n",
    "    \n",
    "    return xk, gk, k, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Fletcher-Reeves | cuadrática\n",
      "N = 2\n",
      "[[-1.2  1. ]]\n",
      "f(x0) = [[2.66]]\n",
      "xk = [0.24999919 0.24999919]\n",
      "||gk|| = 4.601667816800304e-06\n",
      "f(xk) = [[-0.25]]\n",
      "k = 32\n",
      "res = 1\n",
      "\n",
      "Test Fletcher-Reeves | cuadrática\n",
      "N = 10\n",
      "[[-1.2  1.  -1.2  1.  -1.2  1.  -1.2  1.  -1.2  1. ]]\n",
      "f(x0) = [[62.5]]\n",
      "xk = [0.04999993 0.04999993 0.04999993]\n",
      "||gk|| = 4.668968258165778e-06\n",
      "f(xk) = [[-0.25]]\n",
      "k = 50\n",
      "res = 1\n",
      "\n",
      "Test Fletcher-Reeves | cuadrática\n",
      "N = 20\n",
      "[[-1.2  1.  -1.2  1.  -1.2  1.  -1.2  1.  -1.2  1.  -1.2  1.  -1.2  1.\n",
      "  -1.2  1.  -1.2  1.  -1.2  1. ]]\n",
      "f(x0) = [[248.]]\n",
      "xk = [0.02500003 0.02500003 0.02500003]\n",
      "||gk|| = 4.800317012941582e-06\n",
      "f(xk) = [[-0.25]]\n",
      "k = 63\n",
      "res = 1\n",
      "\n",
      "\n",
      "Test Fletcher-Reeves | Rosenbrock\n",
      "N = 2\n",
      "[[-1.2  1. ]]\n",
      "f(x0) = 24.199999999999996\n",
      "xk = [1.00000294 1.00000588]\n",
      "||gk|| = 6.040269183391008e-06\n",
      "f(xk) = 8.656373449210375e-12\n",
      "k = 16816\n",
      "res = 1\n",
      "\n",
      "Test Fletcher-Reeves | Rosenbrock\n",
      "N = 10\n",
      "[[-1.2  1.  -1.2  1.  -1.2  1.  -1.2  1.  -1.2  1. ]]\n",
      "f(x0) = 2057.0\n",
      "xk = [0.99999999 0.99999998 0.99999997]\n",
      "||gk|| = 6.051541949616569e-06\n",
      "f(xk) = 4.8903966157907195e-12\n",
      "k = 21547\n",
      "res = 1\n",
      "\n",
      "Test Fletcher-Reeves | Rosenbrock\n",
      "N = 20\n",
      "[[-1.2  1.  -1.2  1.  -1.2  1.  -1.2  1.  -1.2  1.  -1.2  1.  -1.2  1.\n",
      "  -1.2  1.  -1.2  1.  -1.2  1. ]]\n",
      "f(x0) = 4598.0\n",
      "xk = [1. 1. 1.]\n",
      "||gk|| = 5.9474116387793085e-06\n",
      "f(xk) = 5.4501739383569125e-12\n",
      "k = 23323\n",
      "res = 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pruebas realizadas \n",
    "\n",
    "EPS_M = np.finfo(float).eps\n",
    "MAX_N = 50000\n",
    "TOL = EPS_M ** (1/3)\n",
    "\n",
    "def short(x):\n",
    "    print(np.concatenate((x[0,:3], x[0,-min(3, len(x)-3):]if len(x) > 3 else [])))\n",
    "\n",
    "\n",
    "def test_Fletcher_Reeves(N, f, g, maxN, tol, mess):\n",
    "    x0 = np.ones((1,N))\n",
    "    x0[:,::2] = -1.2\n",
    "    \n",
    "    print(f\"Test Fletcher-Reeves | {mess}\")\n",
    "    print(f'N = {N}')\n",
    "    print(x0)\n",
    "    print(f'f(x0) = {f(x0)}')\n",
    "    \n",
    "    xk, gk, k, res = Fletcher_Reeves(f, g, x0, maxN, tol)\n",
    "    \n",
    "    print('xk = ', end='')\n",
    "    short(xk)\n",
    "    print(f'||gk|| = {np.linalg.norm(gk)}')\n",
    "    print(f'f(xk) = {f(xk)}')\n",
    "    print(f'k = {k}')\n",
    "    print(f'res = {res}')\n",
    "    print()\n",
    "    \n",
    "nn = (2, 10, 20)\n",
    "for n in nn:\n",
    "    test_Fletcher_Reeves(n, cuad, g_cuad, MAX_N, TOL, 'cuadrática')\n",
    "print()\n",
    "for n in nn:\n",
    "    test_Fletcher_Reeves(n, rosenbrock, g_rosenbrock, MAX_N, TOL, 'Rosenbrock')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3 (3.5 puntos)\n",
    "\n",
    "Programar el método de gradiente conjugado no lineal de usando la fórmula de\n",
    "Hestenes-Stiefel:\n",
    "\n",
    "En este caso el algoritmo es igual al del Ejercicio 2, con excepción del cálculo de $\\beta_{k+1}$. Primero se calcula el vector $\\mathbf{y}_k$ y luego $\\beta_{k+1}$:\n",
    "\n",
    "$$ \\mathbf{y}_k =  \\nabla f_{k+1}-\\nabla f_{k} $$\n",
    "$$ \\beta_{k+1} =   \\frac{\\nabla f_{k+1}^\\top\\mathbf{y}_k }{\\nabla p_{k}^\\top\\mathbf{y}_k}  $$\n",
    "\n",
    "1. Repita el Ejercicio 2 usando la fórmula de Hestenes-Stiefel.\n",
    "2. ¿Cuál de los métodos es mejor para encontrar los óptimos de las funciones de prueba?\n",
    "\n",
    "### Solución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En esta celda puede poner el código de las funciones\n",
    "# o poner la instrucción para importarlas de un archivo .py\n",
    "\n",
    "def Hestenes_Stiefel(f, g, x0, maxN, tol):\n",
    "    xk = x0\n",
    "    res = 0\n",
    "    \n",
    "    gnext = g(xk)\n",
    "    pk = -gnext\n",
    "    \n",
    "    for k in range(maxN):\n",
    "        gk = gnext\n",
    "        if np.linalg.norm(gk) < tol:\n",
    "            res = 1\n",
    "            break\n",
    "\n",
    "        alpha = backtracking(f, xk, pk, 0.9, 0.0001)\n",
    "        xk = xk + alpha * pk\n",
    "        \n",
    "        gnext = g(xk)\n",
    "        \n",
    "        yk = gnext - gk\n",
    "        betak = (gnext @ yk.T) / (pk @ yk.T)\n",
    "        pk = -gnext + betak * pk\n",
    "        \n",
    "        if np.abs(gnext @ gk.T) > 0.2 * np.linalg.norm(gnext)**2:\n",
    "            # Reinicio\n",
    "            pk = - gnext\n",
    "    \n",
    "    return xk, gk, k, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Hestenes_Stiefel | cuadrática\n",
      "n = 2\n",
      "f(x0) = [[18.68]]\n",
      "xk = [0.09999974 0.09999974 0.09999974]\n",
      "||gk|| = 5.873056436555319e-06\n",
      "f(xk) = [[-0.25]]\n",
      "k = 96\n",
      "res = 1\n",
      "\n",
      "Test Hestenes_Stiefel | cuadrática\n",
      "n = 10\n",
      "f(x0) = [[18.68]]\n",
      "xk = [0.09999974 0.09999974 0.09999974]\n",
      "||gk|| = 5.873056436555319e-06\n",
      "f(xk) = [[-0.25]]\n",
      "k = 96\n",
      "res = 1\n",
      "\n",
      "Test Hestenes_Stiefel | cuadrática\n",
      "n = 20\n",
      "f(x0) = [[18.68]]\n",
      "xk = [0.09999974 0.09999974 0.09999974]\n",
      "||gk|| = 5.873056436555319e-06\n",
      "f(xk) = [[-0.25]]\n",
      "k = 96\n",
      "res = 1\n",
      "\n",
      "\n",
      "Test Hestenes_Stiefel | Rosenbrock\n",
      "n = 2\n",
      "f(x0) = 1016.4000000000001\n",
      "xk = [0.99999982 0.99999963 0.99999926]\n",
      "||gk|| = 6.025060792347923e-06\n",
      "f(xk) = 2.9892103437503606e-12\n",
      "k = 19971\n",
      "res = 1\n",
      "\n",
      "Test Hestenes_Stiefel | Rosenbrock\n",
      "n = 10\n",
      "f(x0) = 1016.4000000000001\n",
      "xk = [0.99999982 0.99999963 0.99999926]\n",
      "||gk|| = 6.025060792347923e-06\n",
      "f(xk) = 2.9892103437503606e-12\n",
      "k = 19971\n",
      "res = 1\n",
      "\n",
      "Test Hestenes_Stiefel | Rosenbrock\n",
      "n = 20\n",
      "f(x0) = 1016.4000000000001\n",
      "xk = [0.99999982 0.99999963 0.99999926]\n",
      "||gk|| = 6.025060792347923e-06\n",
      "f(xk) = 2.9892103437503606e-12\n",
      "k = 19971\n",
      "res = 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pruebas realizadas \n",
    "\n",
    "EPS_M = np.finfo(float).eps\n",
    "MAX_N = 50000\n",
    "TOL = EPS_M ** (1/3)\n",
    "\n",
    "def test_Hestenes_Stiefel(n, f, g, maxN, tol, mess):\n",
    "    x0 = np.ones((1,N))\n",
    "    x0[:,::2] = -1.2\n",
    "    \n",
    "    print(f\"Test Hestenes_Stiefel | {mess}\")\n",
    "    print(f'n = {n}')\n",
    "    print(f'f(x0) = {f(x0)}')\n",
    "    \n",
    "    xk, gk, k, res = Hestenes_Stiefel(f, g, x0, maxN, tol)\n",
    "    \n",
    "    print('xk = ', end='')\n",
    "    short(xk)\n",
    "    print(f'||gk|| = {np.linalg.norm(gk)}')\n",
    "    print(f'f(xk) = {f(xk)}')\n",
    "    print(f'k = {k}')\n",
    "    print(f'res = {res}')\n",
    "    print()\n",
    "    \n",
    "nn = (2, 10, 20)\n",
    "for n in nn:\n",
    "    test_Hestenes_Stiefel(n, cuad, g_cuad, MAX_N, TOL, 'cuadrática')\n",
    "print()\n",
    "for n in nn:\n",
    "    test_Hestenes_Stiefel(n, rosenbrock, g_rosenbrock, MAX_N, TOL, 'Rosenbrock')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusión\n",
    "\n",
    "Notemos que con ambos métodos se obtuvo un resultado convergente en todas las pruebas, más aún, llegaban al mismo mínimo. La única parte donde se desempeñaron distinto fue en el número de iteraciones. Notemos que el método *Fletcher-Reeves* siempre tuvo menos iteraciones que *Hestenes-Stiefel*. Por lo tanto en estas funciones resulto mejor usar el método *Fletcher-Reeves*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
