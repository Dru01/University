{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curso de Optimización (DEMAT)\n",
    "## Tarea 4\n",
    "\n",
    "| Descripción:                         | Fechas               |\n",
    "|--------------------------------------|----------------------|\n",
    "| Fecha de publicación del documento:  | **Febrero 24, 2022** |\n",
    "| Fecha límite de entrega de la tarea: | **Marzo 6, 2022**    |\n",
    "\n",
    "\n",
    "### Indicaciones\n",
    "\n",
    "Puede escribir el código de los algoritmos que se piden en una\n",
    "celda de este notebook o si lo prefiere, escribir las funciones\n",
    "en un archivo `.py` independiente e importar la funciones para\n",
    "usarlas en este notebook. Lo importante es que en el notebook\n",
    "aparezcan los resultados de la pruebas realizadas y que:\n",
    "\n",
    "- Si se requieren otros archivos para poder reproducir los resultados,\n",
    "  para mandar la tarea cree un archivo ZIP en el que incluya\n",
    "  el notebook y los archivos adicionales. \n",
    "- Si todos los códigos para que se requieren para reproducir los\n",
    "  resultados están en el notebook, no hace falta comprimirlo \n",
    "  y puede anexar sólo el notebook en la tarea del Classroom.\n",
    "- Exportar el notebook a un archivo PDF y anexarlo en la tarea del\n",
    "  Classroom como un archivo independiente.\n",
    "  **No lo incluya dentro del ZIP**, porque la idea que lo pueda accesar \n",
    "  directamente para poner anotaciones y la calificación de cada ejercicio.\n",
    "\n",
    "En la descripción de los ejercicios se nombran algunas variables\n",
    "para el algoritmo, pero sólo es para facilitar la descripción.\n",
    "En la implementación pueden nombrar sus variables como gusten.\n",
    "\n",
    "En los algoritmos se describen las entradas de las\n",
    "funciones. La intención es que tomen en cuenta lo que requiere\n",
    "el algoritmo y que tiene que haber parámetros que permitan\n",
    "controlar el comportamiento del algoritmo,\n",
    "evitando que dejen fijo un valor y que no se puede modificar\n",
    "para hacer diferentes pruebas. Si quieren dar esta información\n",
    "usando un tipo de dato que contenga todos los valores o\n",
    "usar variables por separado, etc., lo pueden hacer y no usen\n",
    "variables globales si no es necesario.\n",
    "\n",
    "Lo mismo para los valores que devuelve una función. \n",
    "Pueden codificar como gusten la manera en que regresa los cálculos.\n",
    "El punto es que podamos tener acceso a los resultados,\n",
    "sin usar variables globales, y que la función no sólo imprima \n",
    "los valores que después no los podamos usar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ejercicio 1 (5 puntos)\n",
    "\n",
    "Programar el método de descenso máximo con tamaño de paso exacto para\n",
    "minimizar funciones cuadráticas:\n",
    "\n",
    "$$ f(x) = \\frac{1}{2} x^\\top \\mathbf{A} x - b^\\top x, $$\n",
    "\n",
    "donde  $\\mathbf{A} \\in  \\mathbb{R}^{n \\times n}$ y $x \\in \\mathbb{R}^n$.\n",
    "\n",
    "Dado el vector $b$,  la matriz $\\mathbf{A}$, \n",
    "un punto inicial $x_0$, un número  máximo de iteraciones $N$, \n",
    "la tolerancia $\\tau>0$. Fijar $k=0$ y repetir los siguientes pasos:\n",
    "\n",
    "1. Calcular el gradiente en el punto $x_k$, \n",
    "\n",
    "   $$g_k = \\nabla f(x_k) = \\mathbf{A} x_k - b.$$\n",
    "2. Si $\\|g_k\\| < \\tau$, entonces $x_k$ es (casi) un punto estacionario.\n",
    "   Hacer $res=1$ y terminar el ciclo. \n",
    "3. Elegir la dirección de descenso como $p_k = - g_k$.\n",
    "4. Calcular el tamaño de paso $\\alpha_{k}$ que minimiza el valor de la función\n",
    "   $$\\phi_k(\\alpha) =  f(x_k + \\alpha p_k)$$\n",
    "   es decir, calcular\n",
    "   $$ \\alpha_{k} = -\\frac{ g_{k}^{\\top} p_{k}}{ p_{k}^{\\top}\\mathbf{A}p_{k}} $$\n",
    "5. Calcular el siguiente punto de la secuencia como\n",
    "   $$x_{k+1} = x_k + \\alpha_k p_k $$\n",
    "6. Si $k+1\\geq N$, hacer $res=0$ y terminar.\n",
    "7. Si no, hacer $k = k+1$ y volver el paso 1.\n",
    "6. Devolver el punto $x_k$, $f_k= \\frac{1}{2} x_k^\\top \\mathbf{A} x_k - b^\\top x_k$, $g_k$, \n",
    "   $k$ y $res$.\n",
    "\n",
    "---\n",
    "\n",
    "1. Escriba una función que implementa el algoritmo anterior \n",
    "   usando arreglos de Numpy.  \n",
    "2. Escriba una función para probar el funcionamiento del método\n",
    "   de descenso máximo. Esta función debe recibir como parámetros\n",
    "   el nombre de un archivo `.npy` que contiene las entradas de\n",
    "   una matriz cuadrada $\\mathbf{A}$, el vector $b$, un punto inicial $x_0$,\n",
    "   el número máximo de iteraciones $N$ y la\n",
    "   tolerancia $\\tau$.   \n",
    "   \n",
    "* Esta función debe crear la matriz $\\mathbf{A}$ y el vector $b$ \n",
    "  leyendo los archivos de datos.\n",
    "* Obtener el número de filas $r$ de la matriz e imprimir este valor.\n",
    "* Compruebe que la matriz es simétrica  y definida positiva calculando\n",
    "  e imprimiendo el valor $\\|\\mathbf{A} - \\mathbf{A}^\\top\\|$ y su\n",
    "  eigenvalor más pequeño (use la función `numpy.linalg.eig()`).\n",
    "* Ejecutar la función del Inciso 1.\n",
    "* Dependiendo del valor de la variable $res$, imprima un mensaje que diga que el \n",
    "  algoritmo convergió ($res=1$) o no ($res=0$).\n",
    "* Imprimir $k$, $f_k$, la norma de $g_k$, y los primeros 3 y últimos 3 elementos del arreglo $x_k$.\n",
    "* Calcule directamente el minimizador resolviendo la ecuación \n",
    "  $Ax_* = b$ e imprima el valor del error $\\|x_k - x_* \\|$.\n",
    "  \n",
    "3. Pruebe la función del Inciso 2  usando $N=1000$, la tolerancia\n",
    "   $\\tau = \\epsilon_m^{1/3}$, donde $\\epsilon_m$ es el\n",
    "   épsilon de la máquina, y los arreglos que se incluyen en \n",
    "   el archivo datosTarea04.zip, de la siguiente manera:\n",
    "\n",
    "| Matriz   | Vector     | Punto para iniciar la secuencia  |\n",
    "|----------|------------|----------------------------------|\n",
    "| A1.npy   | b1.npy     |  $x_0 = (0, -5)$                 |\n",
    "| A1.npy   | b1.npy     |  $x_0 = (7045, 7095)$            |\n",
    "| A2.npy   | b2.npy     |  $x_0 = (0,0,...,0) \\in \\mathbb{R}^{500}$             |\n",
    "| A2.npy   | b2.npy     |  $x_0 = (10000,10000,...,10000) \\in \\mathbb{R}^{500}$ |\n",
    "\n",
    "   \n",
    "### Solución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En esta celda puede poner el código de las funciones\n",
    "# o poner la instrucción para importarlas de un archivo .py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def f_square(x, A, b):\n",
    "    return ((x@A.T)@x.T)/2 - b@x.T\n",
    "\n",
    "def gf_square(x, A, b):\n",
    "    return x@A.T - b\n",
    "\n",
    "def gradient_descent_square_function(f, gf, A, b, x_0, N, T):\n",
    "\n",
    "    x_k = x_0\n",
    "    k = 0\n",
    "    res = 0\n",
    "    while (k < N):\n",
    "        gf_k = gf(x_k, A, b)\n",
    "        if np.linalg.norm(gf_k) < T:\n",
    "            res = 1\n",
    "            break\n",
    "        p_k = -gf_k\n",
    "        a_k = -(gf_k@p_k.T)/(p_k@A@p_k.T)\n",
    "        x_k = x_k + a_k*p_k\n",
    "        k = k + 1\n",
    "\n",
    "    return x_k, f(x_k, A, b), gf(x_k, A, b), k, res\n",
    "\n",
    "def files_data(file_name, x_0, N, T):\n",
    "    \n",
    "    A = np.load(\"A\"+file_name)\n",
    "    print(A.shape)\n",
    "    print(np.linalg.norm(A-A.T),np.min(np.linalg.eigvals(A)))\n",
    "    b = np.load(\"b\"+file_name)\n",
    "    b = np.array([b])\n",
    "    print(b.shape)\n",
    "    x_k, f_k, g_k, k, res = gradient_descent_square_function(f_square, gf_square, A, b, x_0, N, T)\n",
    "    print(f\"El algoritmo \" + (\"no\" if res == 0 else \"\") + \" convergió en:\")\n",
    "    x_output = np.concatenate((x_k[0,:3], x_k[0,-min(3, len(x_k)-3):]if len(x_k) > 3 else []))\n",
    "    print(f\"k = {k}, f_k = {f_k}, ||g_k|| = {np.linalg.norm(g_k)}, x_k = {x_output}\")\n",
    "    print(np.linalg.norm(x_k-np.linalg.solve(A, b.squeeze())))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "0.0 0.10000000000000003\n",
      "(1, 2)\n",
      "El algoritmo  convergió en:\n",
      "k = 69, f_k = [[-62.75]], ||g_k|| = 4.767340293857206e-06, x_k = [-24.49997287  25.49997743]\n",
      "3.5289985397025655e-05\n",
      "(2, 2)\n",
      "0.0 0.10000000000000003\n",
      "(1, 2)\n",
      "El algoritmo  convergió en:\n",
      "k = 1, f_k = [[-62.75]], ||g_k|| = 6.421214021448452e-13, x_k = [-24.5  25.5]\n",
      "9.131096203816022e-13\n",
      "(500, 500)\n",
      "1.5063918215255853e-14 0.09999999999999293\n",
      "(1, 500)\n",
      "El algoritmo  convergió en:\n",
      "k = 332, f_k = [[-5239.54141208]], ||g_k|| = 5.996601774308784e-06, x_k = [-11.61784712   5.44982336   0.96506345]\n",
      "3.9297511566363496e-05\n",
      "(500, 500)\n",
      "1.5063918215255853e-14 0.09999999999999293\n",
      "(1, 500)\n",
      "El algoritmo  convergió en:\n",
      "k = 453, f_k = [[-5239.54141208]], ||g_k|| = 5.7003604586793e-06, x_k = [-11.61784726   5.4498234    0.96506346]\n",
      "3.859553623165663e-05\n"
     ]
    }
   ],
   "source": [
    "# Lectura de datos y pruebas realizadas\n",
    "\n",
    "x_0 = np.array([[[0,-5]],[[7045,7095]]])\n",
    "for x_0_i in x_0:\n",
    "    files_data(\"1.npy\", x_0_i, 1000, np.finfo(float).eps**(1/3))\n",
    "\n",
    "N = 500\n",
    "x_0 = np.array([[np.zeros(N)],[np.repeat(10000,N)]])\n",
    "for x_0_i in x_0:\n",
    "    files_data(\"2.npy\", x_0_i, 1000, np.finfo(float).eps**(1/3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2 (5 puntos)\n",
    "\n",
    "Programar el método de descenso máximo con tamaño de paso seleccionado\n",
    "por la estrategia de backtracking:\n",
    "\n",
    "\n",
    "**Algoritmo de descenso máximo con backtracking:**\n",
    "\n",
    "Dada una función $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$, su gradiente\n",
    "$g: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n$, \n",
    "un punto inicial $x_0$, un número  máximo de iteraciones $N$, \n",
    "una tolerancia $\\tau>0$. Fijar $k=0$ y repetir los siguientes pasos:\n",
    "\n",
    "1. Calcular el gradiente en el punto $x_k$:\n",
    "\n",
    "   $$ g_k = \\nabla f(x_k) = g(x_k) $$\n",
    "2. Si $\\|g_k\\|<\\tau$, $x_k$ es un aproximadamente un punto estacionario, \n",
    "   por lo que hay que hacer $res=1$ y terminar el ciclo.\n",
    "3. Eligir la dirección de descenso como $p_k = - g_k$.\n",
    "4. Calcular el tamaño de paso $\\alpha_k$ mediante la estrategia de backtraking,\n",
    "   usando el algoritmo que describe más adelante.\n",
    "5. Calcular el siguiente punto de la secuencia como\n",
    "   $$x_{k+1} = x_k + \\alpha_k p_k $$\n",
    "6. Si ${k+1}>N$, hacer $res=0$ y terminar. \n",
    "7. Si no, hacer $k = k+1$ y volver el paso 1.\n",
    "8. Devolver el punto $x_k$, $f_k= f(x_k)$, $g_k$, $k$ y $res$.\n",
    "\n",
    "---\n",
    "\n",
    "** Algoritmo de backtracking **\n",
    "\n",
    "Backtracking($f$, $f_k$, $g_k$, $x_k$, $p_k$, $\\alpha_{ini}$, $\\rho$, $c$)\n",
    "\n",
    "El algoritmo recibe la función $f$, el punto $x_k$, $f_k = f(x_k)$, la dirección de descenso $p_k$,\n",
    "un valor inicial $\\alpha_{ini}$, $\\rho \\in (0,1)$, $c \\in (0,1)$.\n",
    "\n",
    "Fijar  $\\alpha = \\alpha_{ini}$  y repetir los siguientes pasos:\n",
    "\n",
    "1.  Si se cumple la condición\n",
    "    $$ f(x_k+\\alpha p_k) \\leq f_k + c \\alpha g_k^\\top p_k, $$ \n",
    "    terminar el ciclo devolviendo \n",
    "2.  Hacer $\\alpha = \\rho \\alpha$ y regresar al paso anterior.\n",
    "\n",
    "---\n",
    "\n",
    "1. Escriba una función que implementa el algoritmo de backtracking. \n",
    "2. Escriba la función que implementa el algoritmo de máximo\n",
    "   descenso con búsqueda inexacta, usando backtraking. \n",
    "   Tiene que recibir como paramétros todos los elementos que se \n",
    "   listaron para ambos algoritmos.\n",
    "3. Escriba una función para probar el funcionamiento del método\n",
    "   de descenso máximo. Esta función debe recibir la función\n",
    "   $f$, la función $g$ que devuelve su gradiente, el punto inicial $x_0$, el número  \n",
    "   máximo de iteraciones $N$, la tolerancia $\\tau>0$ y\n",
    "   el factor $\\rho$ del algoritmo de backtracking.\n",
    "   \n",
    "* Fijar los parámetros $\\alpha_{ini}=2$ y $c=0.0001$ del algoritmo de backtracking.\n",
    "* Ejecutar la función del Inciso 2.\n",
    "* Dependiendo del valor de la variable $res$, imprima un mensaje que diga que el \n",
    "  algoritmo convergió ($res=1$) o no ($res=0$)\n",
    "* Imprimir $k$, $x_k$, $f_k$ y la norma de $g_k$.\n",
    "\n",
    "4. Pruebe la función del Inciso 3  usando $N=10000$, $\\rho=0.8$, la tolerancia\n",
    "   $\\tau = \\epsilon_m^{1/3}$, donde $\\epsilon_m$ es el\n",
    "   épsilon de la máquina. Aplique esta función  a:\n",
    "* La función de Rosenbrock, descrita en la Tarea 3, usando como punto inicial $x_0= (-1.2, 1)$ y $x_0= (-12, 10)$.\n",
    "  Como referencia, el minimizador de la función  es $x_* = (1,1)$.\n",
    "  \n",
    "5. Repita el inciso anterior con $\\rho=0.5$.   \n",
    "\n",
    "   \n",
    "### Solución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En esta celda puede poner el código de las funciones\n",
    "# o poner la instrucción para importarlas de un archivo .py\n",
    "\n",
    "def f_rosenbrock(x):\n",
    "    return 100*(x[0,1] - x[0,0]**2)**2 + (1 - x[0,0])**2\n",
    "\n",
    "def gf_rosenbrock(x):\n",
    "    return np.array( [[400*(x[0,0]**3-x[0,0]*x[0,1])+2*(x[0,0]-1), 200*(x[0,1]-x[0,0]**2)]] )\n",
    "\n",
    "def backtracking(f, f_k, gf_k, x_k, p_k, alpha, ro, c):\n",
    "    while (f(x_k + alpha*p_k) > f_k + c*alpha*gf_k@p_k.T):\n",
    "        alpha = ro*alpha\n",
    "    return alpha \n",
    "\n",
    "def gradient_descent_backtracking(f, gf, x_0, alpha, ro, c, N, T):\n",
    "\n",
    "    x_k = x_0\n",
    "    k = 0\n",
    "    res = 0\n",
    "    while (k < N):\n",
    "        gf_k = gf(x_k)\n",
    "        if np.linalg.norm(gf_k) < T:\n",
    "            res = 1\n",
    "            break\n",
    "        p_k = -gf_k\n",
    "        a_k = backtracking(f, f(x_k), gf_k, x_k, p_k, alpha, ro, c)\n",
    "        x_k = x_k + a_k*p_k\n",
    "        k = k + 1\n",
    "\n",
    "    return x_k, f(x_k), gf(x_k), k, res\n",
    "\n",
    "def exercise(f, gf, x_0, N, T, ro):\n",
    "    \n",
    "    x_k, f_k, g_k, k, res = gradient_descent_backtracking(f, gf, x_0, 2, ro, 0.0001, N, T)\n",
    "    print(f\"El algoritmo \" + (\"no\" if res == 0 else \"\") + \" convergió en:\")\n",
    "    x_output = np.concatenate((x_k[0,:3], x_k[0,-min(3, len(x_k)-3):]if len(x_k) > 3 else []))\n",
    "    print(f\"k = {k}, f_k = {f_k}, ||g_k|| = {np.linalg.norm(g_k)}, x_k = {x_output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El algoritmo no convergió en:\n",
      "k = 10000, f_k = 6.274640640397673e-07, ||g_k|| = 0.001941845100290949, x_k = [1.00079208 1.00158391]\n",
      "El algoritmo no convergió en:\n",
      "k = 10000, f_k = 8.531110165045911, ||g_k|| = 3.1564912743986095, x_k = [ 3.92075554 15.37404809]\n"
     ]
    }
   ],
   "source": [
    "# Lectura de datos y pruebas realizadas\n",
    "\n",
    "x_0 = np.array([[[-1.2,1]],[[-12,10]]])\n",
    "for x_0_i in x_0:\n",
    "    exercise(f_rosenbrock, gf_rosenbrock, x_0_i, 10000, np.finfo(float).eps**(1/3), 0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
