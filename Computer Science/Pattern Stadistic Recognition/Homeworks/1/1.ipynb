{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT\n",
    "\n",
    "### Problema\n",
    "\n",
    "![image.png](C:\\Users\\batma\\OneDrive\\Documents\\Academic\\School\\University\\Computer Science\\Pattern Stadistic Recognition\\Homeworks\\1\\hola.png)\n",
    "\n",
    "¿Cómo calificarías la respuesta de ChatGPT?\n",
    "¿Cuál es tu respuesta? Demuéstrala formalmente.\n",
    "\n",
    "### Respuestas\n",
    "\n",
    "Aunque la respuesta y la intuición es correcta, no lo es la justificación.\n",
    "\n",
    "A pesar de que la matriz de covarianza solo mide las relaciones lineales entre variables no necesariamentre es independiente de como esten expresadas las variables. Puesto que estas pueden no ser una transformación lineal una de la otra, como es nuestro caso.\n",
    "\n",
    "Sin embargo si se cumple en nuestro caso, veamos lo siguiente. Sean $X$ y $Y$ v.a. que miden la temperatura en difernetes partes del cuerpo en grados Celcius y grados Fahrenheit respectivamente. Recordemos que\n",
    "\n",
    "$$Y = \\frac{9}{5}X + 32.$$\n",
    "\n",
    "Puesto que la varianza es invariante bajo translaciones y $Var(aX) = a^2Var(X)$.\n",
    "\n",
    "$$Var(Y) = Var(\\frac{9}{5}X + 32) = \\frac{81}{25}Var(X),$$\n",
    "\n",
    "es decir la varianza de $Y$ es proporcional a la varianza de $X$, por lo tanto maximizar la varianza de $X$ es análogo a maximizar la varianza de $Y$,\n",
    "\n",
    "$$\\max_l \\frac{Var(l^tX)}{||l||^2} \\Longleftrightarrow \\max_l \\frac{Var(l^tY)}{||l||^2},$$\n",
    "\n",
    "con lo que concluimos que ambos obtendran la misma dirección de máxima varianza."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Estandar\n",
    "\n",
    "\n",
    "### Problema\n",
    "\n",
    "Supongamos que $X=(X_1,X_2), Var(X_1)=Var(X_2)=1, E[X_1]=E[X_2]=1, y X_1\\perp X_2$. Demuetra que cualquier dirección $l$ da máxima varianza en las proyecciones.\n",
    "\n",
    "### Demostración\n",
    "\n",
    "Por definición de $Cov(X)$ obtenemos\n",
    "\n",
    "$$Cov(X) = \\begin{pmatrix}\n",
    "    Var(X_1) & Cov(X_1,X_2)\\\\\n",
    "    Cov(X_2,X_1) & Var(X_2)\\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    1 & 0\\\\\n",
    "    0 & 1\\\\\n",
    "\\end{pmatrix},$$\n",
    "\n",
    "por lo tanto\n",
    "\n",
    "$$\\max_l \\frac{l^tCov(X)l}{||l||^2} = \\max_l \\frac{l^tl}{||l||^2} = \\max_l 1,$$\n",
    "\n",
    "lo cual es independiente de $l$. Con lo que concluimos cualquier dirección $l$ da máxima varianza en las proyecciones."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demuestra que $K = -\\frac{1}{2}CD^2C$\n",
    "\n",
    "Recordemos que\n",
    "\n",
    "- $K$ : Es la matriz de producto punto entre variables, es decir $K_{i,j} = X_i \\dot X_j$.\n",
    "- $D^2$ : Es la matriz de distancias al cuadrado entre variables, es decir $D^2_{i,j} = d(X_i,X_j)^2$.\n",
    "- $C$ : Es la matriz identidad menos 1/n sobre todas las entradas, es decir $C = I - \\frac{1}{n}J$.\n",
    "\n",
    "También en clase se demostro que\n",
    "\n",
    "$$K = -\\frac{1}{2}CD^2C$$\n",
    "$$K_{i,j} = -\\frac{1}{2} \\left(D^2_{i,j} - \\frac{\\sum_j D^2_{i,j}}{n} + \\frac{\\sum_j K_{i,j}}{n} - \\frac{\\sum_i D^2_{i,j}}{n} + \\frac{\\sum_i K_{i,j}}{n}\\right)$$\n",
    "$$\\sum_i \\sum_j D^2_{i,j} = 2n\\sum_i K_{i,i}$$\n",
    "\n",
    "por lo que nos falta demostrar que\n",
    "\n",
    "$$CD^2C = D^2_{i,j} - \\frac{1}{n}\\sum_j D^2_{i,j} - \\frac{1}{n}\\sum_i D^2_{i,j} + \\frac{1}{n^2} \\sum_i \\sum_j D^2_{i,j}.$$\n",
    "\n",
    "Para ello usaremos la notación $X_{i,\\cdot}$ y $X_{\\cdot, j}$, para denotar al vector renglón y columna respectivamente. Ahora veamos que\n",
    "\n",
    "\\begin{align}\n",
    "    (CD^2C)_{i,j} &= (CD^2)_{i,\\cdot}C_{\\cdot,j} &\\text{Por definición de multiplicación de matrices}\\\\\n",
    "    &= \\sum_k (CD^2)_{i,k}(C)_{k,j} &\\text{Por definición de multiplicación de matrices}\\\\\n",
    "    &= \\sum_k (C_{i,\\cdot}D^2_{\\cdot,k})C_{k,j} &\\text{Por definición de multiplicación de matrices}\\\\\n",
    "    &= \\sum_k (\\sum_h C_{i,h}D^2_{h,k})C_{k,j} &\\text{Por definición de multiplicación de matrices}\\\\\n",
    "    &= \\sum_k (D^2_{i,k} - \\sum_h \\frac{1}{n}D^2_{h,k})C_{k,j} &\\text{Por definición de C}\\\\\n",
    "    &= \\sum_k (D^2_{i,k} - \\frac{1}{n} \\sum_h D^2_{h,k})C_{k,j}\\\\\n",
    "    &= (D^2_{i,j} - \\frac{1}{n} \\sum_h D^2_{h,j}) + \\sum_k -\\frac{1}{n}(D^2_{i,k} - \\frac{1}{n} \\sum_h D^2_{h,k}) &\\text{Por definición de C}\\\\\n",
    "    &= D^2_{i,j} - \\frac{1}{n} \\sum_h D^2_{h,j} - \\frac{1}{n}\\sum_k (D^2_{i,k} - \\frac{1}{n} \\sum_h D^2_{h,k})\\\\\n",
    "    &= D^2_{i,j} - \\frac{1}{n} \\sum_i D^2_{i,j} - \\frac{1}{n}\\sum_j D^2_{i,j} + \\frac{1}{n^2} \\sum_i \\sum_j D^2_{i,j}\\\\\n",
    "\\end{align}\n",
    "\n",
    "Por lo tanto concluimos la demostración."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plano de direcciones PCA\n",
    "\n",
    "### Problema\n",
    "\n",
    "Revisa el video sobre la maximización del cociente de Rayleigh: https://youtu.be/8TBpSUXcDww.\n",
    "\n",
    "Haz unos pequeños cambios necesarios para demostrar que el segundo vector propio de Cov(X) es la solución del problema de maximizar el cociente bajo la restricción adicional de ser ortogonalal primer vector propio.\n",
    "\n",
    "### Demostración\n",
    "\n",
    "Primero recordemos lo siguiente de la demostración. Sea $A$ una matriz simétrica entonces\n",
    "\n",
    "- Su SVD es $A = U \\Lambda U^t$, donde $U^t = U^-1$ y $\\Lambda$ es una matriz diagonal con los valores propios de $A$.\n",
    "- Definimos a $\\Lambda^{1/2} := Diag(\\sqrt{\\mu_i}),$ donde $\\mu_i$ son los valores propios de $A$, entonces podemos ver que $\\Lambda = \\Lambda^{1/2}\\Lambda^{1/2}$ y por lo tanto $(U\\Lambda^{1/2}U^t)(U\\Lambda^{1/2}U^t) = A^{1/2}A^{1/2} = A$.\n",
    "- El vector dirección que maximiza la varianza es $U^te_1 = v_1$ el primer vector propio de $Cov(X)$ con valor $\\mu_1$.\n",
    "- Debido a que $U$ es una matriz ortonormal entonces conserva ortgonalidades entonces $l_1^tl=0$ ssi $(U^tl_1)^t(U^tl)=0$ ssi $e_1^ty=0$.\n",
    "- Por último, si $v^te_1 = 0$ es ssi $\\sum v_ie_i = 0$ lo cúal es ssi $v_1 = 0$.\n",
    "\n",
    "Entonces podemos ver que\n",
    "\n",
    "\\begin{align}\n",
    "    \\max_{l : l_1^tl=0}  \\frac{l^t Cov(X) l}{l^t l} &= \\max_{l : l_1^tl=1}  \\frac{l^t (U\\Lambda^{1/2}U^t)(U\\Lambda^{1/2}U^t) l}{l^tl} & \\text{Puesto que } Cov(X) \\text{ es simétrica}\\\\\n",
    "    &= \\max_{l : l_1^tl=0}  \\frac{l^t (U\\Lambda^{1/2}U^t)(U\\Lambda^{1/2}U^t) l}{l^t (UU^t) l} & \\text{Ya que } U \\text{ es una matriz unitaria}\\\\\n",
    "    &= \\max_{y : y^te_1=0}  \\frac{y \\Lambda y^t}{y^t y} & \\text{Con el cambio de variable } y = U^tl\\\\\n",
    "    &= \\max_{y : y^te_1=0}  \\frac{\\sum_{i=1}^{n} \\mu_i y_i^2}{\\sum_{i=1}^{n} y_i^2} & \\text{Por definición de } \\Lambda\\\\\n",
    "    &= \\max_{y}  \\frac{\\sum_{i=2}^{n} \\mu_i y_i^2}{\\sum_{i=2}^{n} y_i^2} & \\text{Ya que } y^te_1 = 0 \\text{ entonces } y_1 = 0\\\\\n",
    "    &\\leq \\max_{y}  \\frac{\\sum_{i=1}^{n} \\mu_2 y_i^2}{\\sum_{i=1}^{n} y_i^2} & \\text{Debido a que } \\mu_2 \\geq \\mu_i, \\forall i>2\\\\\n",
    "    &= \\max_{y}  \\mu_2\\\\\n",
    "\\end{align}\n",
    "\n",
    "Ahora veamos que si $y = (0,1,\\cdots,0)$ entonces\n",
    "\n",
    "$$\\frac{\\sum \\mu_i y_i^2}{\\sum y_i^2} = \\frac{\\mu_2}{1} = \\mu_2.$$\n",
    "\n",
    "Por lo tanto concluimos que $v_2$ el segundo vector propio de $Cov(X)$, es la dirección que maximiza la varianza y que además es ortgonal a $v_1$, cuyo máximo es $\\mu_2$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización de Datos\n",
    "\n",
    "### Problema\n",
    "\n",
    "En el archivo heptatlon se pueden consultar los tiempos y el puntaje final (score) de 25 atletas que participaron en el heptatlon durante los juegos olmpicos de Seoul.\n",
    "\n",
    "a) Busca visualizaciones informativas de estos datos multivariados.\n",
    "\n",
    "b) Haz un análisis de componentes principales con los tiempos (sinscore). Hay una relación entre el score y las proyecciones sobre el primer CP? Puedes leer los datos con: d¡-read.table(”hepatlon”)\n",
    "\n",
    "### Solución\n",
    "\n",
    "a) Renombramos (hurdles,highjump,shot,run200m,longjump,javelin,run800m,score) como (V1, V2, V3, V4, V5, V6, V7, V8)\n",
    "\n",
    "Distribuciones conjuntas a pares de (V_i,V_j) con $i\\neq j$ y distirbución margina discreta $V_i$ cuando $i=j$.\n",
    "\n",
    "![image.png](C:\\Users\\batma\\OneDrive\\Documents\\Academic\\School\\University\\Computer Science\\Pattern Stadistic Recognition\\Homeworks\\1\\output.png)\n",
    "\n",
    "Distribuciones conjuntas a pares de (V_i,V_j) con $i\\neq j$ y distirbución margina continua $V_i$ cuando $i=j$.\n",
    "\n",
    "![image.png](C:\\Users\\batma\\OneDrive\\Documents\\Academic\\School\\University\\Computer Science\\Pattern Stadistic Recognition\\Homeworks\\1\\output1.png)\n",
    "\n",
    "Podemos ver en el biplot que v7, v1 y v4 están fuertemente relacionada con el PC1 e inversamente relacionadas v2, v5, v8, v3. En cambio la v6 está fuertemente relacionada con PC2. Así mismo podemos ver que la hay bastante correlación entre v7, v1 y v4, asi como bastante correlación entre v2, v5, v8, v3. \n",
    "\n",
    "![image.png](C:\\Users\\batma\\OneDrive\\Documents\\Academic\\School\\University\\Computer Science\\Pattern Stadistic Recognition\\Homeworks\\1\\output2.png)\n",
    "\n",
    "b)\n",
    "\n",
    "[ 1.19624885e-03, -1.05204013e-04, -2.09849444e-03,  1.47499229e-03, -7.92784821e-04, -1.57865298e-03,  1.12710147e-02, -9.99930909e-01]\n",
    "\n",
    "[ 1.92835321e-02, -6.05680149e-04,  8.27323011e-02, -1.73321242e-02,  1.08856631e-03,  3.27484685e-01,  9.40817259e-01,  9.91075581e-03]\n",
    "\n",
    "[-4.42281031e-02,  5.20709703e-03,  5.23034341e-02,  2.52425088e-02,  3.33247513e-02, -9.42375189e-01,  3.24711313e-01,  4.99545357e-03]\n",
    "\n",
    "[ 1.27735048e-01, -3.39503633e-02,  9.78341573e-01,  1.16494549e-01, -5.42199304e-02,  1.74323988e-02, -9.25021755e-02, -2.75215967e-03]\n",
    "\n",
    "[ 2.99253782e-02, -6.54426901e-02,  1.10968349e-01, -9.90594181e-01,  8.48590375e-03, -2.83264257e-02, -1.87933943e-02, -1.82525884e-03]\n",
    "\n",
    "[-9.77750566e-01,  1.11049607e-02,  1.19861716e-01, -1.94015933e-02, -1.64765138e-01,  4.41739357e-02, -6.02027169e-03, -1.45801494e-03]\n",
    "\n",
    "[ 1.56298683e-01,  7.45409331e-02, -6.80079688e-02, -1.54259545e-02, -9.81458446e-01, -3.97783256e-02,  1.75083617e-02,  1.33740223e-03]"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Rubén Pérez Palacios Lic. Computación Matemática"
   },
   {
    "name": "Profesor: Johan Van Horebeek"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "title": "Reconocimiento Estadístico de Patrones - Tarea 1",
  "vscode": {
   "interpreter": {
    "hash": "c347c8f9a7ef94e4c9e03b4513be7835ed18f45b99a2a817fb579f408b867b16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
